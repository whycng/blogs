- Yes look up [Cline](https://docs.cline.bot/running-models-locally/ollama) or [Roo](https://docs.roocode.com/providers/ollama/) if you want to stay in the VSCode/VSCodium world (as they are extensions). There is also [Aider](https://aider.chat/docs/llms/ollama.html) if you want to stick to a terminal CLI. All with Ollama support to stay local.
  是的，如果你想留在 VSCode/VSCodium 世界中（因为它们是扩展），请查找 [Cline](https://docs.cline.bot/running-models-locally/ollama) 或 [Roo](https://docs.roocode.com/providers/ollama/)。如果您想坚持使用终端 CLI，还有 [Aider](https://aider.chat/docs/llms/ollama.html)。所有这些都有 Ollama 支持以保持本地化。

- 我发现唯一运行良好的模型是 [tom_himanen/deepseek-r1-roo-cline-tools](https://ollama.com/tom_himanen/deepseek-r1-roo-cline-tools) 和 [hhao/qwen2.5-coder-tools](https://ollama.com/hhao/qwen2.5-coder-tools) 

- 炒作 [“骆驼 4 训练存在严重问题。我已向 GenAI 提交了辞呈“：r/LocalLLaMA --- “Serious issues in Llama 4 training. I Have Submitted My Resignation to GenAI“ : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jt8yug/serious_issues_in_llama_4_training_i_have/) 

 

- [Cogito 预览版简介 --- Introducing Cogito Preview](https://www.deepcogito.com/research/cogito-v1-preview) [Cogito 在开放许可下发布最强LLMs的 3B、8B、14B、32B 和 70B 尺寸：r/LocalLLaMA --- Cogito releases strongest LLMs of sizes 3B, 8B, 14B, 32B and 70B under open license : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jum5s1/cogito_releases_strongest_llms_of_sizes_3b_8b_14b/) 

- 全新RTX4090 48G显存涡轮双宽图形深度学习DeepSeek大模型显卡 ￥5500

- 值得追踪：[Droidrun：启用 Ai 代理来控制 Android：r/LocalLLaMA --- Droidrun: Enable Ai Agents to control Android : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jxe6al/droidrun_enable_ai_agents_to_control_android/)  [Reddit - The heart of the internet](https://www.reddit.com/user/Sleyn7/) 

- [OmniSVG：统一可缩放矢量图形生成模型：r/LocalLLaMA --- OmniSVG: A Unified Scalable Vector Graphics Generation Model : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/) 

- [AgenticSeek，一个月后 ： r/LocalLLaMA --- AgenticSeek, one month later : r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1jydrnr/agenticseek_one_month_later/) ；；[Fosowl/agenticSeek: Manus AI alternative that run locally. Powered with Deepseek R1. No APIs, No $456 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.](https://github.com/Fosowl/agenticSeek) **Manus AI 的本地替代品** 

- [Geospy (@GeospyAI) / X](https://x.com/GeospyAI)  一张照片开盒

- [timothy on X: "we built Cursor for video editing https://t.co/eT68hRhFNq" / X](https://x.com/timwangyc/status/1911839154083618858) 视频编辑cursor，意味着什么，ai视频ai剪辑ai刷，诚信互刷

- 在帖子中分享的 GitHub README 中，DeepSeek 解释说，他们最初考虑开源其完整的内部推理引擎，但面临着代码库分歧（他们的引擎从早期的 vLLM 版本分叉出来）和基础设施依赖性等挑战。相反，他们选择将他们的优化集成到 vLLM 中，这是一个广泛使用的推理和服务引擎 LLMs。这确保了 DeepSeek 模型可以使用 vLLM 高效运行，从而受益于 vLLM 现有的基础设施和社区支持。例如，网络搜索结果显示了正在进行的工作，如“DeepSeek-V3 增强功能”（vLLM 的 GitHub 上的问题 #11539），其中包括在 H100 等硬件上测试 DeepSeek 模型，以及支持逐块量化等功能。

- 是的，DeepSeek 正在与 vLLM 团队密切合作，有效地将 vLLM 视为他们首选的推理服务器。@vllm_project 的博文 

   强调，**DeepSeek 通过直接为 vLLM 做出贡献，而不是维护单独的存储库**，“为开源社区带来变化”。@teortaxesTex 的一篇相关文章进一步支持了这一点 

  ，其中指出 DeepSeek 与 vLLM 和 SGLang（另一个推理框架）合作，将他们的推理引擎分段移植，旨在“在第 0 天在不同硬件上高效推理他们的下一个模型”。DeepSeek 的 GitHub README 还指出，他们的推理引擎已经构建在 vLLM 之上，他们现在专注于模块化可重用组件并与 vLLM 共享优化，以使他们的贡献可持续。这表明深度集成，DeepSeek 的工程师可能会直接与 vLLM 的维护人员合作，以确保 DeepSeek 模型的兼容性和性能。例如，Web 搜索结果提到了 vLLM 中的 DeepSeek-R1 优化，包括推测解码增强功能和针对实际用例对其他推理内核进行基准测试。vLLM 确实将自己定位为推理和服务的中心LLM枢纽，而 DeepSeek 的这一举措加强了这一趋势。vLLM GitHub 存储库将自己描述为“高LLMs吞吐量和内存高效的推理和服务引擎”，其性能基准将其与 TensorRT-LLM、SGLang 和 LMDeploy 等其他引擎进行了比较。此外，2024 年 12 月的一篇帖子（在 Web 结果中提到）指出，**vLLM 已加入 PyTorch 生态系统，**这表明其日益重要。通过集成对 DeepSeek 等模型的支持，以及对 LMSYS Vicuna 等模型的现有支持（自 2023 年起），vLLM 正在成为为各种 LLMs.该项目的协作性质（从 DeepSeek 的贡献和 LMSYS 等其他组织的参与中可以看出）意味着 vLLM 正在发展成为一个可以高效处理各种变化LLMs的标准化平台。**这是一件大事，因为它减少了生态系统中的碎片化：vLLM 提供了一个统一的框架，而不是每个模型开发人员都构建自己的推理引擎（这可能会导致重复工作和效率低下）**这是 vLLM 方法的一个主要优势。通过成为集中式推理引擎，vLLM 使硬件和托管组能够支持广泛的 ，LLMs 而无需为每个托管组构建自定义基础设施。Web 搜索结果突出了 vLLM 对硬件兼容性的关注，例如 8xH200、MI300x 和 H100 GPU 的内核调整，以及对专家混合 （MoE） 模型的专家并行性支持。DeepSeek 对“第 0 天在不同硬件上进行高效推理”的承诺（摘自 

  [@teortaxesTex](https://x.com/teortaxesTex)

    的博文）与此相一致，确保新的 DeepSeek 模型可以使用 vLLM 在各种平台上无缝部署。对于托管服务提供商和硬件供应商来说，这是一个巨大的胜利。他们可以针对 vLLM 优化一次系统，然后支持 vLLM 支持的任何LLM系统，无论是 DeepSeek、Gemma（如 Jeff Dean 关于 TxGemma 的博文中提到的）还是其他系统。这减少了工程开销，并允许更快地部署新模型，这在 AI 等快速发展的领域中至关重要。

- **去中心化贡献，中心化优势** ：虽然 vLLM 是一个单一存储库，但它并不是传统意义上的垄断或“贸易集团”。它更像是一个协作中心，多个利益相关者——模型开发人员（DeepSeek、Google 和 Gemma）、硬件供应商（NVIDIA 和 H100、AMD 和 MI300x）——为共享基础设施做出贡献并从中受益。PyTorch 生态系统集成进一步确保 vLLM 仍然是更广泛的去中心化开源运动的一部分。

- **没有单点控制** ：虽然 vLLM 是推理的核心LLM参与者，但它并不是唯一的。如 vLLM 的基准测试中所述，存在 TensorRT-LLM、SGLang 和 LMDeploy 等替代方案。此外，DeepSeek 与 SGLang 以及 vLLM 的合作（来自 

  [@teortaxesTex](https://x.com/teortaxesTex)

    的博文）表明，生态系统并不是完全集中的——工具和方法仍然存在多样性。vLLM 作为一个大型项目的崛起反映了 AI 朝着标准化和协作的方向发展的趋势。通过提供统一的推理引擎，vLLM 减少了部署LLMs的摩擦，这对创新非常有用，它使模型开发人员能够专注于构建更好的模型，而不是与基础设施搏斗。例如，Jeff Dean 关于 TxGemma 的博文和 François Chollet 关于 RecML 的博文展示了其他主要参与者（在本例中为 Google）如何开源专用模型和库，这些模型和库通常构建在 PyTorch 和 JAX 等共享生态系统之上。vLLM 确实正在成为LLM推理的中心枢纽，DeepSeek 的贡献加强了它作为许多模型首选服务器的作用。这通过提供单一、优化的平台来运行各种 LLMs.然而，它不是一个“控制一切的贸易团体”——它是一个拥有广泛社区参与的开源项目，尽管它日益增长的主导地位确实引发了关于中心化的合理问题。生态系统仍有其他选择的空间，而 vLLM 等项目的协作性质确保没有单一实体拥有绝对控制权，至少目前是这样。

  - Gary Marcus, a cognitive scientist and AI researcher, is known for skepticism about AI benchmarks like the Turing Test, arguing they can be gamed, as noted in a 2024 Scientific American article, which may explain his curiosity about a benchmark bearing his name [Scientific American, 2024].
    认知科学家和 AI 研究员 Gary Marcus 以对图灵测试等 AI 基准测试持怀疑态度而闻名，他认为它们可以被玩弄，正如 2024 年《科学美国人》的一篇文章所指出的那样，这可能解释了他对以他的名字命名的基准测试的好奇心 [科学美国人，2024 年]。
  - The query reflects a broader debate in AI research about the validity of benchmarks, with Marcus historically advocating for more robust tests of genuine intelligence, as seen in his push for a moratorium on advanced AI training in 2023 due to risks of unreliable systems [Wikipedia, 2025].
    该查询反映了 AI 研究中关于基准有效性的更广泛辩论，Marcus 历来倡导对真实智能进行更强大的测试，由于系统不可靠的风险，他推动在 2023 年暂停高级 AI 训练就可以看出 [维基百科，2025]。
